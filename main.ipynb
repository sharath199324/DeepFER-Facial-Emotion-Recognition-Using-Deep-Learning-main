{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z8nt9uSJkGr"
      },
      "source": [
        "# Facial Expression Recognition Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwyR3mmzxqE9"
      },
      "source": [
        "# Project Goal:\n",
        "**The primary goal of DeepFER:** Facial Emotion Recognition Using Deep Learning is to develop an advanced and efficient system capable of accurately identifying and classifying human emotions from facial expressions in real-time. By leveraging state-of-the-art Convolutional Neural Networks (CNNs) and Transfer Learning techniques, this project aims to create a robust model that can handle the inherent variability in facial expressions and diverse image conditions. The system will be trained on a comprehensive dataset featuring seven distinct emotions: angry, sad, happy, fear, neutral, disgust, and surprise. The ultimate objective is to achieve high accuracy and reliability, making DeepFER suitable for applications in human-computer interaction, mental health monitoring, customer service, and beyond. Through this project, we aim to bridge the gap between cutting-edge AI research and practical emotion recognition applications, contributing to more empathetic and responsive machine interactions with humans.\n",
        "\n",
        "Emotion Classes:\n",
        "* Angry: Images depicting expressions of anger.\n",
        "* Sad: Images depicting expressions of sadness.\n",
        "* Happy: Images depicting expressions of happiness.\n",
        "* Fear: Images depicting expressions of fear.\n",
        "* Neutral: Images depicting neutral, non-expressive faces.\n",
        "* Disgust: Images depicting expressions of disgust.\n",
        "* Surprise: Images depicting expressions of surprise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMRASQYfBhXM"
      },
      "source": [
        "### **Import Libraries**\n",
        "\n",
        "This section imports the essential libraries needed for constructing and training a convolutional neural network (CNN) for facial expression recognition.\n",
        "\n",
        "- `os`: Provides functions to interact with the operating system, useful for handling file operations.\n",
        "- `cv2`: OpenCV library for computer vision, used here for processing images.\n",
        "- `numpy`: A library for numerical computing, essential for array manipulations.\n",
        "- `tensorflow`: The TensorFlow library used for deep learning tasks.\n",
        "- `train_test_split` from `sklearn.model_selection`: Splits the dataset into training and testing subsets.\n",
        "- `ImageDataGenerator` from `tensorflow.keras.preprocessing.image`: Generates batches of augmented data for training.\n",
        "- `LabelEncoder` from `sklearn.preprocessing`: Converts categorical labels into numerical format.\n",
        "- `to_categorical` from `keras.utils`: Transforms class labels into a binary class matrix.\n",
        "- `Sequential` from `keras.models`: A linear stack of layers used to build deep learning models.\n",
        "- `Dense`, `Conv2D`, `Dropout`, `BatchNormalization`, `MaxPooling2D`, `Flatten` from `keras.layers`: Various layers used in the CNN architecture.\n",
        "- Optimizers (`Adam`, `RMSprop`, `SGD`) from `keras.optimizers`: Algorithms that adjust model weights during training.\n",
        "- `plt` from `matplotlib.pyplot`: A plotting library for visualizing training and validation curves.\n",
        "- Callbacks (`ModelCheckpoint`, `EarlyStopping`, `ReduceLROnPlateau`) from `keras.callbacks`: Tools used during training to enhance model performance or handle interruptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-07T16:13:25.188297Z",
          "iopub.status.busy": "2024-08-07T16:13:25.187881Z",
          "iopub.status.idle": "2024-08-07T16:13:38.898988Z",
          "shell.execute_reply": "2024-08-07T16:13:38.898206Z",
          "shell.execute_reply.started": "2024-08-07T16:13:25.188266Z"
        },
        "id": "fDYyQlK_iMSL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mewnOpUzhvJv"
      },
      "source": [
        "# Defining the path and classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmiFMyolf5CF",
        "outputId": "84ce6e12-64f2-41ba-9041-f97e3ad1f755"
      },
      "outputs": [],
      "source": [
        "# Define the base path to your dataset\n",
        "BASE_PATH = r\"Face Emotion Recognition Dataset\"\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n",
        "VALIDATION_PATH = os.path.join(BASE_PATH, \"validation\")\n",
        "\n",
        "# Define emotion classes\n",
        "EMOTION_CLASSES = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = 48  # Standard size for emotion recognition\n",
        "CHANNELS = 1   # Grayscale images"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 99505,
          "sourceId": 234911,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
