{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z8nt9uSJkGr"
      },
      "source": [
        "# Facial Expression Recognition Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwyR3mmzxqE9"
      },
      "source": [
        "# Project Goal:\n",
        "**The primary goal of DeepFER:** Facial Emotion Recognition Using Deep Learning is to develop an advanced and efficient system capable of accurately identifying and classifying human emotions from facial expressions in real-time. By leveraging state-of-the-art Convolutional Neural Networks (CNNs) and Transfer Learning techniques, this project aims to create a robust model that can handle the inherent variability in facial expressions and diverse image conditions. The system will be trained on a comprehensive dataset featuring seven distinct emotions: angry, sad, happy, fear, neutral, disgust, and surprise. The ultimate objective is to achieve high accuracy and reliability, making DeepFER suitable for applications in human-computer interaction, mental health monitoring, customer service, and beyond. Through this project, we aim to bridge the gap between cutting-edge AI research and practical emotion recognition applications, contributing to more empathetic and responsive machine interactions with humans.\n",
        "\n",
        "Emotion Classes:\n",
        "* Angry: Images depicting expressions of anger.\n",
        "* Sad: Images depicting expressions of sadness.\n",
        "* Happy: Images depicting expressions of happiness.\n",
        "* Fear: Images depicting expressions of fear.\n",
        "* Neutral: Images depicting neutral, non-expressive faces.\n",
        "* Disgust: Images depicting expressions of disgust.\n",
        "* Surprise: Images depicting expressions of surprise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMRASQYfBhXM"
      },
      "source": [
        "### **Import Libraries**\n",
        "\n",
        "This section imports the essential libraries needed for constructing and training a convolutional neural network (CNN) for facial expression recognition.\n",
        "\n",
        "- `os`: Provides functions to interact with the operating system, useful for handling file operations.\n",
        "- `cv2`: OpenCV library for computer vision, used here for processing images.\n",
        "- `numpy`: A library for numerical computing, essential for array manipulations.\n",
        "- `tensorflow`: The TensorFlow library used for deep learning tasks.\n",
        "- `train_test_split` from `sklearn.model_selection`: Splits the dataset into training and testing subsets.\n",
        "- `ImageDataGenerator` from `tensorflow.keras.preprocessing.image`: Generates batches of augmented data for training.\n",
        "- `LabelEncoder` from `sklearn.preprocessing`: Converts categorical labels into numerical format.\n",
        "- `to_categorical` from `keras.utils`: Transforms class labels into a binary class matrix.\n",
        "- `Sequential` from `keras.models`: A linear stack of layers used to build deep learning models.\n",
        "- `Dense`, `Conv2D`, `Dropout`, `BatchNormalization`, `MaxPooling2D`, `Flatten` from `keras.layers`: Various layers used in the CNN architecture.\n",
        "- Optimizers (`Adam`, `RMSprop`, `SGD`) from `keras.optimizers`: Algorithms that adjust model weights during training.\n",
        "- `plt` from `matplotlib.pyplot`: A plotting library for visualizing training and validation curves.\n",
        "- Callbacks (`ModelCheckpoint`, `EarlyStopping`, `ReduceLROnPlateau`) from `keras.callbacks`: Tools used during training to enhance model performance or handle interruptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-07T16:13:25.188297Z",
          "iopub.status.busy": "2024-08-07T16:13:25.187881Z",
          "iopub.status.idle": "2024-08-07T16:13:38.898988Z",
          "shell.execute_reply": "2024-08-07T16:13:38.898206Z",
          "shell.execute_reply.started": "2024-08-07T16:13:25.188266Z"
        },
        "id": "fDYyQlK_iMSL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mewnOpUzhvJv"
      },
      "source": [
        "# Defining the path and classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmiFMyolf5CF",
        "outputId": "84ce6e12-64f2-41ba-9041-f97e3ad1f755"
      },
      "outputs": [],
      "source": [
        "# Define the base path to your dataset\n",
        "BASE_PATH = r\"Face Emotion Recognition Dataset\"\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n",
        "VALIDATION_PATH = os.path.join(BASE_PATH, \"validation\")\n",
        "\n",
        "# Define emotion classes\n",
        "EMOTION_CLASSES = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "# Image parameters\n",
        "IMG_SIZE = 48  # Standard size for emotion recognition\n",
        "CHANNELS = 1   # Grayscale images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-07T16:13:38.980114Z",
          "iopub.status.busy": "2024-08-07T16:13:38.979772Z",
          "iopub.status.idle": "2024-08-07T16:13:38.984423Z",
          "shell.execute_reply": "2024-08-07T16:13:38.983539Z",
          "shell.execute_reply.started": "2024-08-07T16:13:38.980066Z"
        },
        "id": "OBRZU_fzisqq",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "LOADING TRAINING DATA\n",
            "==================================================\n",
            "Loading angry images...\n",
            "Loaded 3993 angry images\n",
            "Loading disgust images...\n",
            "Loaded 436 disgust images\n",
            "Loading fear images...\n",
            "Loaded 4103 fear images\n",
            "Loading happy images...\n",
            "Loaded 7164 happy images\n",
            "Loading neutral images...\n",
            "Loaded 4982 neutral images\n",
            "Loading sad images...\n",
            "Loaded 4938 sad images\n",
            "Loading surprise images...\n",
            "Loaded 3205 surprise images\n"
          ]
        }
      ],
      "source": [
        "# Load training dataset\n",
        "# Initialize lists for training data\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"LOADING TRAINING DATA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for emotion_class in EMOTION_CLASSES:\n",
        "    emotion_path = os.path.join(TRAIN_PATH, emotion_class)\n",
        "    \n",
        "    if not os.path.exists(emotion_path):\n",
        "        print(f\"Warning: Path {emotion_path} does not exist!\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"Loading {emotion_class} images...\")\n",
        "    emotion_images = os.listdir(emotion_path)\n",
        "    \n",
        "    # Loop through each image in the emotion folder\n",
        "    for image_name in emotion_images:\n",
        "        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(emotion_path, image_name)\n",
        "            \n",
        "            # Load image in grayscale\n",
        "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "            \n",
        "            if img is not None:\n",
        "                # Resize image to target size\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                \n",
        "                # Normalize pixel values to [0, 1]\n",
        "                img = img.astype('float32') / 255.0\n",
        "                \n",
        "                # Add to lists\n",
        "                train_images.append(img)\n",
        "                train_labels.append(emotion_class)\n",
        "    \n",
        "    print(f\"Loaded {len([l for l in train_labels if l == emotion_class])} {emotion_class} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-08-07T16:16:08.424977Z",
          "iopub.status.busy": "2024-08-07T16:16:08.424704Z",
          "iopub.status.idle": "2024-08-07T16:16:08.488509Z",
          "shell.execute_reply": "2024-08-07T16:16:08.487581Z",
          "shell.execute_reply.started": "2024-08-07T16:16:08.424953Z"
        },
        "id": "t3ehOepji55a",
        "outputId": "73883aa0-71a3-48be-ac23-903ab8ae76e8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training data loaded!\n",
            "Training images shape: (28821, 48, 48)\n",
            "Training labels shape: (28821,)\n"
          ]
        }
      ],
      "source": [
        "# Convert to numpy arrays\n",
        "X_train = np.array(train_images)\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "print(f\"\\nTraining data loaded!\")\n",
        "print(f\"Training images shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "LOADING VALIDATION DATA\n",
            "==================================================\n",
            "Loading angry images...\n",
            "Loaded 960 angry images\n",
            "Loading disgust images...\n",
            "Loaded 111 disgust images\n",
            "Loading fear images...\n",
            "Loaded 1018 fear images\n",
            "Loading happy images...\n",
            "Loaded 1825 happy images\n",
            "Loading neutral images...\n",
            "Loaded 1216 neutral images\n",
            "Loading sad images...\n",
            "Loaded 1139 sad images\n",
            "Loading surprise images...\n",
            "Loaded 797 surprise images\n",
            "\n",
            "Validation data loaded!\n",
            "Validation images shape: (7066, 48, 48)\n",
            "Validation labels shape: (7066,)\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists for validation data\n",
        "val_images = []\n",
        "val_labels = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING VALIDATION DATA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Loop through each emotion class in validation data\n",
        "for emotion_class in EMOTION_CLASSES:\n",
        "    emotion_path = os.path.join(VALIDATION_PATH, emotion_class)\n",
        "    \n",
        "    if not os.path.exists(emotion_path):\n",
        "        print(f\"Warning: Path {emotion_path} does not exist!\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"Loading {emotion_class} images...\")\n",
        "    emotion_images = os.listdir(emotion_path)\n",
        "    \n",
        "    # Loop through each image in the emotion folder\n",
        "    for image_name in emotion_images:\n",
        "        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(emotion_path, image_name)\n",
        "            \n",
        "            # Load image in grayscale\n",
        "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "            \n",
        "            if img is not None:\n",
        "                # Resize image to target size\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                \n",
        "                # Normalize pixel values to [0, 1]\n",
        "                img = img.astype('float32') / 255.0\n",
        "                \n",
        "                # Add to lists\n",
        "                val_images.append(img)\n",
        "                val_labels.append(emotion_class)\n",
        "    \n",
        "    print(f\"Loaded {len([l for l in val_labels if l == emotion_class])} {emotion_class} images\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_val = np.array(val_images)\n",
        "y_val = np.array(val_labels)\n",
        "\n",
        "print(f\"\\nValidation data loaded!\")\n",
        "print(f\"Validation images shape: {X_val.shape}\")\n",
        "print(f\"Validation labels shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images reshaped for CNN:\n",
            "Training data shape: (28821, 48, 48, 1)\n",
            "Validation data shape: (7066, 48, 48, 1)\n"
          ]
        }
      ],
      "source": [
        "# Reshape images to add channel dimension for CNN\n",
        "X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
        "X_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
        "\n",
        "print(\"Images reshaped for CNN:\")\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images reshaped for CNN:\n",
            "Training data shape: (28821, 48, 48, 1)\n",
            "Validation data shape: (7066, 48, 48, 1)\n"
          ]
        }
      ],
      "source": [
        "# Reshape images to add channel dimension for CNN\n",
        "X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
        "X_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
        "\n",
        "print(\"Images reshaped for CNN:\")\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 99505,
          "sourceId": 234911,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
